services:
  # PostgreSQL for Airflow metadata
  postgres:
    image: postgres:15-alpine
    environment:
      POSTGRES_USER: airflow
      POSTGRES_PASSWORD: airflow
      POSTGRES_DB: airflow
    volumes:
      - postgres_data:/var/lib/postgresql/data
    healthcheck:
      test: ["CMD", "pg_isready", "-U", "airflow"]
      interval: 5s
      retries: 5
    networks:
      - iptu-network

  # Apache Spark Master
  spark-master:
    image: apache/spark:3.5.7-scala2.12-java11-python3-r-ubuntu
    container_name: spark-master
    command: /opt/spark/bin/spark-class org.apache.spark.deploy.master.Master
    environment:
      - SPARK_MASTER_HOST=spark-master
      - SPARK_MASTER_PORT=7077
    ports:
      - "8080:8080"
      - "7077:7077"
    volumes:
      - ./data:/opt/data
      - ./outputs:/opt/outputs
      - ./logs:/opt/logs
    networks:
      - iptu-network

  # Apache Spark Worker
  spark-worker:
    image: apache/spark:3.5.7-scala2.12-java11-python3-r-ubuntu
    container_name: spark-worker
    depends_on:
      - spark-master
    command: /opt/spark/bin/spark-class org.apache.spark.deploy.worker.Worker spark://spark-master:7077
    environment:
      - SPARK_WORKER_MEMORY=2g
      - SPARK_WORKER_CORES=2
    volumes:
      - ./data:/opt/data
      - ./outputs:/opt/outputs
      - ./logs:/opt/logs
    networks:
      - iptu-network

  # Airflow Database Init
  airflow-init:
    image: apache/airflow:2.8.0-python3.11
    container_name: airflow-init
    depends_on:
      postgres:
        condition: service_healthy
    environment:
      - AIRFLOW__CORE__EXECUTOR=LocalExecutor
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://airflow:airflow@postgres/airflow
      - AIRFLOW__CORE__FERNET_KEY=''
    volumes:
      - ./requirements.txt:/opt/airflow/requirements.txt
      - ./requirements-pyspark.txt:/opt/airflow/requirements-pyspark.txt
      - ./install-requirements.sh:/opt/airflow/install-requirements.sh
    entrypoint: /bin/bash
    command:
      - -c
      - |
        chmod +x /opt/airflow/install-requirements.sh
        /opt/airflow/install-requirements.sh
        airflow db init
        airflow users create --username admin --firstname Admin --lastname User --role Admin --email admin@example.com --password admin
    networks:
      - iptu-network

  # Airflow Webserver (Official Apache image)
  airflow-webserver:
    image: apache/airflow:2.8.0-python3.11
    container_name: airflow-webserver
    depends_on:
      postgres:
        condition: service_healthy
      airflow-init:
        condition: service_completed_successfully
    environment:
      - AIRFLOW__CORE__EXECUTOR=LocalExecutor
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://airflow:airflow@postgres/airflow
      - AIRFLOW__CORE__FERNET_KEY=''
      - AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION=true
      - AIRFLOW__CORE__LOAD_EXAMPLES=false
      - AIRFLOW__CORE__ENABLE_XCOM_PICKLING=true
      - SPARK_MASTER_URL=spark://spark-master:7077
      - SKIP_PYSPARK=true
    volumes:
      - ./dags:/opt/airflow/dags
      - ./logs:/opt/airflow/logs
      - ./plugins:/opt/airflow/plugins
      - ./data:/opt/airflow/data
      - ./outputs:/opt/airflow/outputs
      - ./src:/opt/airflow/src
      - ./requirements.txt:/opt/airflow/requirements.txt
      - ./requirements-pyspark.txt:/opt/airflow/requirements-pyspark.txt
      - ./install-requirements.sh:/opt/airflow/install-requirements.sh
    entrypoint: /bin/bash
    command:
      - -c
      - |
        chmod +x /opt/airflow/install-requirements.sh
        /opt/airflow/install-requirements.sh
        exec /usr/bin/dumb-init -- /entrypoint webserver
    ports:
      - "8081:8080"
    healthcheck:
      test: ["CMD", "curl", "--fail", "http://localhost:8080/health"]
      interval: 30s
      timeout: 300s
      start_period: 300s
      retries: 10
    networks:
      - iptu-network

  # Airflow Scheduler (Official Apache image)
  airflow-scheduler:
    image: apache/airflow:2.8.0-python3.11
    container_name: airflow-scheduler
    depends_on:
      postgres:
        condition: service_healthy
      airflow-init:
        condition: service_completed_successfully
      airflow-webserver:
        condition: service_healthy
    environment:
      - AIRFLOW__CORE__EXECUTOR=LocalExecutor
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://airflow:airflow@postgres/airflow
      - AIRFLOW__CORE__FERNET_KEY=''
      - AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION=true
      - AIRFLOW__CORE__LOAD_EXAMPLES=false
      - AIRFLOW__CORE__ENABLE_XCOM_PICKLING=true
      - SPARK_MASTER_URL=spark://spark-master:7077
    volumes:
      - ./dags:/opt/airflow/dags
      - ./logs:/opt/airflow/logs
      - ./plugins:/opt/airflow/plugins
      - ./data:/opt/airflow/data
      - ./outputs:/opt/airflow/outputs
      - ./src:/opt/airflow/src
      - ./requirements.txt:/opt/airflow/requirements.txt
      - ./requirements-pyspark.txt:/opt/airflow/requirements-pyspark.txt
      - ./install-requirements.sh:/opt/airflow/install-requirements.sh
    entrypoint: /bin/bash
    command:
      - -c
      - |
        chmod +x /opt/airflow/install-requirements.sh
        /opt/airflow/install-requirements.sh
        exec /usr/bin/dumb-init -- /entrypoint scheduler
    networks:
      - iptu-network

volumes:
  postgres_data:

networks:
  iptu-network:
    driver: bridge
