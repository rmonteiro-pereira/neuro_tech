services:
  # PostgreSQL for Airflow metadata
  postgres:
    image: postgres:15-alpine
    environment:
      POSTGRES_USER: airflow
      POSTGRES_PASSWORD: airflow
      POSTGRES_DB: airflow
    volumes:
      - postgres_data:/var/lib/postgresql/data
    healthcheck:
      test: ["CMD", "pg_isready", "-U", "airflow"]
      interval: 5s
      retries: 5
    networks:
      - iptu-network

  # Apache Spark Master
  spark-master:
    image: apache/spark:3.5.7-scala2.12-java11-python3-r-ubuntu
    container_name: spark-master
    command: /opt/spark/bin/spark-class org.apache.spark.deploy.master.Master
    environment:
      - SPARK_MASTER_HOST=spark-master
      - SPARK_MASTER_PORT=7077
    ports:
      - "8080:8080"
      - "7077:7077"
    volumes:
      - ./data:/opt/data
      - ./outputs:/opt/outputs
      - ./logs:/opt/logs
    networks:
      - iptu-network

  # Apache Spark Worker
  spark-worker:
    image: apache/spark:3.5.7-scala2.12-java11-python3-r-ubuntu
    container_name: spark-worker
    depends_on:
      - spark-master
    command: /opt/spark/bin/spark-class org.apache.spark.deploy.worker.Worker spark://spark-master:7077
    environment:
      - SPARK_WORKER_MEMORY=2g
      - SPARK_WORKER_CORES=2
    volumes:
      - ./data:/opt/data
      - ./outputs:/opt/outputs
      - ./logs:/opt/logs
    networks:
      - iptu-network

  # Airflow Database Init
  airflow-init:
    build: .
    container_name: airflow-init
    depends_on:
      postgres:
        condition: service_healthy
    environment:
      - AIRFLOW__CORE__EXECUTOR=LocalExecutor
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://airflow:airflow@postgres/airflow
      - AIRFLOW__CORE__FERNET_KEY=''
    entrypoint: /bin/bash
    command:
      - -c
      - |
        # Dependencies and Java are pre-installed in the image
        # Just initialize Airflow database and create admin user (if it doesn't exist)
        airflow db init
        # Create or update admin user with password 'admin'
        # Try to create user first, if it fails (user exists), update password instead
        if ! airflow users create --username admin --firstname Admin --lastname User --role Admin --email admin@example.com --password admin 2>/dev/null; then
          echo "User admin already exists, updating password..."
          airflow users update --username admin --password admin --email admin@example.com || true
        fi
        airflow pools set default_pool 128 "Default pool" || true
    networks:
      - iptu-network

  # Airflow Webserver
  airflow-webserver:
    build: .
    container_name: airflow-webserver
    depends_on:
      postgres:
        condition: service_healthy
      airflow-init:
        condition: service_completed_successfully
    environment:
      - AIRFLOW__CORE__EXECUTOR=LocalExecutor
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://airflow:airflow@postgres/airflow
      - AIRFLOW__CORE__FERNET_KEY=''
      - AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION=true
      - AIRFLOW__CORE__LOAD_EXAMPLES=false
      - AIRFLOW__CORE__ENABLE_XCOM_PICKLING=true
      - AIRFLOW__CORE__DEFAULT_POOL_SIZE=128
      - SPARK_MASTER_URL=spark://spark-master:7077
      - PYTHONUSERBASE=/home/airflow/.local
      - PYTHONNOUSERSITE=0
      - PYTHONPATH=/home/airflow/.local/lib/python3.11/site-packages
      - SKIP_PYSPARK=true
    volumes:
      - ./dags:/opt/airflow/dags
      - ./logs:/opt/airflow/logs
      - ./plugins:/opt/airflow/plugins
      - ./data:/opt/airflow/data
      - ./outputs:/opt/airflow/outputs
      - ./src:/opt/airflow/src
    # Dependencies and Java are pre-installed in the image
    entrypoint: /bin/bash
    command:
      - -c
      - |
        # JAVA_HOME and dependencies are already set in the image
        # Just start the webserver
        exec /usr/bin/dumb-init -- /entrypoint webserver
    ports:
      - "8081:8080"
    healthcheck:
      test: ["CMD", "curl", "--fail", "http://localhost:8080/health"]
      interval: 30s
      timeout: 300s
      start_period: 300s
      retries: 10
    networks:
      - iptu-network

  # Airflow Scheduler
  airflow-scheduler:
    build: .
    container_name: airflow-scheduler
    depends_on:
      postgres:
        condition: service_healthy
      airflow-init:
        condition: service_completed_successfully
      airflow-webserver:
        condition: service_healthy
    environment:
      - AIRFLOW__CORE__EXECUTOR=LocalExecutor
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://airflow:airflow@postgres/airflow
      - AIRFLOW__CORE__FERNET_KEY=''
      - AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION=true
      - AIRFLOW__CORE__LOAD_EXAMPLES=false
      - AIRFLOW__CORE__ENABLE_XCOM_PICKLING=true
      - AIRFLOW__CORE__DEFAULT_POOL_SIZE=128
      - SPARK_MASTER_URL=spark://spark-master:7077
      - PYTHONUSERBASE=/home/airflow/.local
      - PYTHONNOUSERSITE=0
      - PYTHONPATH=/home/airflow/.local/lib/python3.11/site-packages
    volumes:
      - ./dags:/opt/airflow/dags
      - ./logs:/opt/airflow/logs
      - ./plugins:/opt/airflow/plugins
      - ./data:/opt/airflow/data
      - ./outputs:/opt/airflow/outputs
      - ./src:/opt/airflow/src
    # Dependencies and Java are pre-installed in the image
    entrypoint: /bin/bash
    command:
      - -c
      - |
        # JAVA_HOME and dependencies are already set in the image
        # Just start the scheduler
        exec /usr/bin/dumb-init -- /entrypoint scheduler
    networks:
      - iptu-network

volumes:
  postgres_data:

networks:
  iptu-network:
    driver: bridge
